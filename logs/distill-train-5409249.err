When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/training_args.py:1913: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
No config specified, defaulting to the single config: openwebtext/plain_text
Using custom data configuration plain_text-31a8eb8e26a3d2e8
Loading Dataset Infos from /mnt/petrelfs/fanyuyu/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521
[INFO|configuration_utils.py:691] 2025-08-29 12:24:37,429 >> loading configuration file /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B/config.json
[INFO|configuration_utils.py:765] 2025-08-29 12:24:37,430 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.51.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2058] 2025-08-29 12:24:37,432 >> loading file chat_template.jinja
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][INFO|tokenization_utils_base.py:2323] 2025-08-29 12:24:37,855 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|modeling_utils.py:1121] 2025-08-29 12:24:37,870 >> loading weights file /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B/model.safetensors.index.json
[INFO|configuration_utils.py:1142] 2025-08-29 12:24:37,873 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:43<02:11, 43.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:43<02:11, 43.96s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.24s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.30s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:12, 44.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:29<01:29, 44.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:30<01:30, 45.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:30<01:30, 45.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:29<01:30, 45.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:29<01:30, 45.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:29<01:30, 45.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:29<01:30, 45.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:30<01:30, 45.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:14<00:44, 44.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:14<00:45, 45.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:15<00:45, 45.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:15<00:45, 45.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:14<00:45, 45.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:15<00:45, 45.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:14<00:45, 45.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:15<00:45, 45.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 31.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.44s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 31.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 31.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.50s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 31.62s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 31.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 31.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 31.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 36.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.49s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 36.51s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 31.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:26<00:00, 36.53s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.47s/it]
[INFO|modeling_utils.py:4930] 2025-08-29 12:27:03,775 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4938] 2025-08-29 12:27:03,775 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-08-29 12:27:03,777 >> loading configuration file /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B/generation_config.json
[INFO|configuration_utils.py:1142] 2025-08-29 12:27:03,778 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|modeling_utils.py:1121] 2025-08-29 12:27:03,780 >> loading weights file /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B/model.safetensors.index.json
[INFO|configuration_utils.py:1142] 2025-08-29 12:27:03,782 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.94s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.47s/it]
[INFO|modeling_utils.py:4930] 2025-08-29 12:27:13,709 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4938] 2025-08-29 12:27:13,709 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1095] 2025-08-29 12:27:13,713 >> loading configuration file /mnt/lustrenew/mllm_safety-shared/models/huggingface/meta-llama/Meta-Llama-3.1-8B/generation_config.json
[INFO|configuration_utils.py:1142] 2025-08-29 12:27:13,713 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "temperature": 0.6,
  "top_p": 0.9
}

Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.48s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.79s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.75s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:04,  4.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.97s/it]
[rank0]:[W829 12:27:16.134996844 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank6]:[W829 12:27:16.230291200 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank7]:[W829 12:27:16.544634979 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W829 12:27:16.550371472 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W829 12:27:16.558998009 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank5]:[W829 12:27:16.578867388 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W829 12:27:16.582275494 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank4]:[W829 12:27:17.903338782 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
[INFO|trainer.py:698] 2025-08-29 12:35:16,885 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:748] 2025-08-29 12:35:16,885 >> Using auto half precision backend
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py:313: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WatermarkLogitsDistillTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
[INFO|trainer.py:2414] 2025-08-29 12:36:05,191 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-08-29 12:36:05,191 >>   Num examples = 320,000
[INFO|trainer.py:2416] 2025-08-29 12:36:05,191 >>   Num Epochs = 9,223,372,036,854,775,807
[INFO|trainer.py:2417] 2025-08-29 12:36:05,191 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:2420] 2025-08-29 12:36:05,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2421] 2025-08-29 12:36:05,191 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2422] 2025-08-29 12:36:05,191 >>   Total optimization steps = 5,000
[INFO|trainer.py:2423] 2025-08-29 12:36:05,192 >>   Number of trainable parameters = 1,003,782,656
[INFO|integration_utils.py:831] 2025-08-29 12:36:05,193 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: lingjiechen127 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/wandb/run-20250829_123605-e0nr116f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run /mnt/lustrenew/mllm_safety-shared/tmp/fanyuyu/models/watermark/meta-llama/Meta-Llama-3.1-8B-logit-watermark-distill-kgw-k2-gamma0.25-delta2
wandb: ⭐️ View project at https://wandb.ai/lingjiechen127/huggingface
wandb: 🚀 View run at https://wandb.ai/lingjiechen127/huggingface/runs/e0nr116f
  0%|          | 0/5000 [00:00<?, ?it/s][WARNING|logging.py:328] 2025-08-29 12:36:07,645 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,645 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,645 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,645 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,651 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,651 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,651 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[WARNING|logging.py:328] 2025-08-29 12:36:07,651 >> `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank4]:     main()
[rank4]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank4]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank4]:     self.optimizer.step()
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank4]:     self.scaler.step(self.optimizer, closure)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank4]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank4]:     retval = optimizer.step(*args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank4]:     return method(*args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank4]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank4]:     ret = func(self, *args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank4]:     adamw(
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank4]:     return func(*args, **kwargs)
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank4]:     func(
[rank4]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank4]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 4 has a total capacity of 79.32 GiB of which 53.56 MiB is free. Including non-PyTorch memory, this process has 79.27 GiB memory in use. Of the allocated memory 76.55 GiB is allocated by PyTorch, and 357.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank7]:     main()
[rank7]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank7]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank7]:     self.optimizer.step()
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank7]:     self.scaler.step(self.optimizer, closure)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank7]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank7]:     retval = optimizer.step(*args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank7]:     return method(*args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank7]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank7]:     ret = func(self, *args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank7]:     adamw(
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank7]:     return func(*args, **kwargs)
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank7]:     func(
[rank7]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank7]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 104.00 MiB. GPU 7 has a total capacity of 79.32 GiB of which 101.56 MiB is free. Including non-PyTorch memory, this process has 79.22 GiB memory in use. Of the allocated memory 76.75 GiB is allocated by PyTorch, and 244.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank2]:     main()
[rank2]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank2]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank2]:     self.scaler.step(self.optimizer, closure)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank2]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank2]:     retval = optimizer.step(*args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank2]:     return method(*args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank2]:     adamw(
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank2]:     func(
[rank2]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank2]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 2 has a total capacity of 79.32 GiB of which 25.56 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 76.63 GiB is allocated by PyTorch, and 299.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank6]:     main()
[rank6]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank6]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank6]:     self.optimizer.step()
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank6]:     self.scaler.step(self.optimizer, closure)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank6]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank6]:     retval = optimizer.step(*args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank6]:     return method(*args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank6]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank6]:     ret = func(self, *args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank6]:     adamw(
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank6]:     func(
[rank6]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank6]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 6 has a total capacity of 79.32 GiB of which 97.56 MiB is free. Including non-PyTorch memory, this process has 79.23 GiB memory in use. Of the allocated memory 76.45 GiB is allocated by PyTorch, and 415.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank5]:     main()
[rank5]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank5]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank5]:     self.optimizer.step()
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank5]:     self.scaler.step(self.optimizer, closure)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank5]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank5]:     retval = optimizer.step(*args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank5]:     return method(*args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank5]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank5]:     ret = func(self, *args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank5]:     adamw(
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank5]:     return func(*args, **kwargs)
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank5]:     func(
[rank5]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank5]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 5 has a total capacity of 79.32 GiB of which 35.56 MiB is free. Including non-PyTorch memory, this process has 79.29 GiB memory in use. Of the allocated memory 76.65 GiB is allocated by PyTorch, and 268.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank3]:     main()
[rank3]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank3]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank3]:     self.optimizer.step()
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank3]:     self.scaler.step(self.optimizer, closure)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank3]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank3]:     retval = optimizer.step(*args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank3]:     return method(*args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank3]:     ret = func(self, *args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank3]:     adamw(
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank3]:     func(
[rank3]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank3]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB. GPU 3 has a total capacity of 79.32 GiB of which 25.56 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 76.63 GiB is allocated by PyTorch, and 300.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank1]:     main()
[rank1]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank1]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank1]:     self.scaler.step(self.optimizer, closure)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank1]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank1]:     retval = optimizer.step(*args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank1]:     return method(*args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank1]:     adamw(
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank1]:     func(
[rank1]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank1]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 1 has a total capacity of 79.32 GiB of which 25.56 MiB is free. Including non-PyTorch memory, this process has 79.30 GiB memory in use. Of the allocated memory 76.63 GiB is allocated by PyTorch, and 299.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
    main()
  File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
    self.optimizer.step()
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
    return method(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 11.56 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 76.81 GiB is allocated by PyTorch, and 273.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 915, in <module>
[rank0]:     main()
[rank0]:   File "/mnt/petrelfs/fanyuyu/lingjie_tmp/WAPITI-Code-Base-master/train_logit_distill.py", line 859, in main
[rank0]:     train_result = trainer.train(resume_from_checkpoint=checkpoint)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/transformers/trainer.py", line 2611, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank0]:     self.scaler.step(self.optimizer, closure)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank0]:     retval = optimizer.step(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank0]:     return method(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 220, in step
[rank0]:     adamw(
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank0]:     func(
[rank0]:   File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank0]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 11.56 MiB is free. Including non-PyTorch memory, this process has 79.31 GiB memory in use. Of the allocated memory 76.81 GiB is allocated by PyTorch, and 273.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0829 12:36:15.140000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14105 closing signal SIGTERM
W0829 12:36:15.143000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14107 closing signal SIGTERM
W0829 12:36:15.144000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14109 closing signal SIGTERM
W0829 12:36:15.145000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14110 closing signal SIGTERM
W0829 12:36:15.145000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14111 closing signal SIGTERM
W0829 12:36:15.146000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14112 closing signal SIGTERM
W0829 12:36:15.146000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 14113 closing signal SIGTERM
E0829 12:36:16.212000 14072 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 14108) of binary: /mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/bin/python3.10
Traceback (most recent call last):
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/mnt/petrelfs/fanyuyu/miniconda3/envs/visual-stitching/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_logit_distill.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-29_12:36:15
  host      : SH-IDCA1404-10-140-54-10
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 14108)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: SH-IDCA1404-10-140-54-10: task 0: Exited with exit code 1
